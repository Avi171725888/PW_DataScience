{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e563cb4-0627-4984-8bb3-634550752ecb",
   "metadata": {},
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26742ac9-9e6f-489a-870c-c0b1ff9b8748",
   "metadata": {},
   "source": [
    "1.Ordinary Least Squares (OLS): ordinary least squares (OLS) is a technique used to calculate the parameters of a linear regression model. The objective is to find the best-fit line that minimizes the sum of squared residuals between the observed data points and the anticipated values from the linear model.\n",
    "2.Ridge Regression: Ridge Regression is a technique used in linear regression to address the problem of overfitting. It does this by adding a regularization term to the loss function, which shrinks the coefficients toward zero. This reduces the variance of the model and can improve its predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762009c7-d1bf-4339-8d01-070e810423d1",
   "metadata": {},
   "source": [
    "Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25e749b-63b5-4f2f-a280-451f9b7b07ff",
   "metadata": {},
   "source": [
    "The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and independence. However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumed.\n",
    "\n",
    "Now, let’s take an example of a linear regression problem and see how ridge regression if implemented, helps us to reduce the error.\n",
    "\n",
    "We shall consider a data set on Food restaurants trying to find the best combination of food items to improve their sales in a particular region. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e74a2-9449-47de-a464-c4c93c9c0914",
   "metadata": {},
   "source": [
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf40ce8-b1f8-4d13-b6d5-8b87e93c5c13",
   "metadata": {},
   "source": [
    "In ridge and lasso linear regression, an important step is to choose the tuning parameter lambda, often I use grid search on log scale from -6->4, it works well on ridge, but on lasso, should I take into account the order of magnitude of output y ? for example, if output y is in nano scale (-9), my search scope for log lambda may be -15 -> -5.\n",
    "all the input parameters are normalized, they're inside -3,3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1796a-8118-41c0-b029-1c0dddd6c089",
   "metadata": {},
   "source": [
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d24d4c-f37a-4c2d-9a28-c8e193fe733e",
   "metadata": {},
   "source": [
    "ne of the most important things about ridge regression is that without wasting any information about predictions it tries to determine variables that have exactly zero effects. Ridge regression is popular because it uses regularization for making predictions and regularization is intended to resolve the problem of overfitting. We mainly find that overfitting is where the size of data is very large and ridge regression works by penalizing the coefficient of features and it also minimizes the errors in prediction. \n",
    "\n",
    "Talking about the other methods of feature importance they directly nullify the effect of less competent features. If we believe that some of the coefficients have zero effect using ridge regression we can build a better model by providing Bayesian priors for regression coefficients. For example, if we know that any of the coefficients have zero effect but which one has it we don’t know then we can use a prior with a ridge to know about the effect of every coefficient.  Let’s see how we can implement ridge for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663297fa-c990-49c5-a1f9-3878a3fbf7ad",
   "metadata": {},
   "source": [
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc2bf7-efc9-4e5a-a4c4-d04cdb537fa7",
   "metadata": {},
   "source": [
    "Multicollinearity:-\n",
    "Multicollinearity, or collinearity, is the existence of near-linear relationships among the independent variables.\n",
    "Effects of Multicollinearity:-\n",
    "Multicollinearity can create inaccurate estimates of the regression coefficients, inflate the standard errors of the\n",
    "regression coefficients, deflate the partial t-tests for the regression coefficients, give false, nonsignificant, p-values, and degrade the predictability of the model (and that’s just for starters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe14ca-a98a-4dc3-9956-e0778d2877f8",
   "metadata": {},
   "source": [
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad1af4-ce47-4e05-b242-783c2e453992",
   "metadata": {},
   "source": [
    "Within SPSS there are two general commands that you can use for analyzing data with a continuous dependent variable and one or more categorical predictors, the regression command and the glm command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d163f49-242f-4efe-a13e-e263ff57d6f5",
   "metadata": {},
   "source": [
    "Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca36936-7653-4c02-9c8e-5c20ca52c92d",
   "metadata": {},
   "source": [
    "the ridge coefficients are a reduced factor of the simple linear regression coefficients and thus never attain zero values but very small values.\n",
    "The Ridge Regression improves the efficiency, but the model is less interpretable due to the potentially high number of features. It performs better in cases where there may be high multi-colinearity, or high correlation between certain features. This is because it reduces variance in exchange for bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e9e66-9915-43a0-b1fd-4282735747d4",
   "metadata": {},
   "source": [
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1fb1e-04b3-4b17-979e-619ae975826a",
   "metadata": {},
   "source": [
    "Yes Ridge Regression can be used for time series data because it can handles continious data and and solve the problem of overfitting.Assimple linear regression can be used for time series data analysis but results are not promising."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
