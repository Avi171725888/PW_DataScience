{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cec34c0-b6b4-418c-b91f-3d4277fe3b7e",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b6e05-164d-4569-9a66-54e2868c2b9a",
   "metadata": {},
   "source": [
    "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.\n",
    "\n",
    "Lasso Regression uses L1 regularization technique (will be discussed later in this article). It is used when we have more features because it automatically performs feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d9162-9ab4-4ff1-b1db-7bc583acff32",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88bbab-65ec-4fc4-9cdc-cfc831ae5ac0",
   "metadata": {},
   "source": [
    "Advantages of LASSO regression:-\n",
    "1.Automatic features selection:-The main advantage of a LASSO regression model is that it has the ability to set the coefficients for features it does not consider interesting to zero. This means that the model does some automatic feature selection to decide which features should and should not be included on its own.\n",
    "2.Reduced overfitting:- Another advantage of a LASSO regression is that the L1 penalty that is added to the model helps to prevent the model from overfitting. This makes intuitive sense because when the model sets feature coefficients to zero and effectively removes features from the model, model complexity decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0180ed8c-0bd6-4e30-8250-07b2fb7cc86a",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5725eb5-fe6f-498e-8c79-ca08a8ef3312",
   "metadata": {},
   "source": [
    "Step 1: Check the Model’s Coefficients\n",
    "The first step in interpreting the results of a Lasso regression model is to examine the values of the model’s coefficients. The coefficients represent the strength and direction of the relationship between the features and the target variable.\n",
    "\n",
    "In Lasso regression, some of the coefficients will be set to zero, which means that the corresponding feature has been excluded from the model. The non-zero coefficients represent the features that are most important for predicting the target variable.\n",
    "\n",
    "Step 2: Check the Model’s Performance Metrics\n",
    "The second step in interpreting the results of a Lasso regression model is to check the model’s performance metrics. These metrics provide an indication of how well the model is performing on the test data set.\n",
    "\n",
    "The most common performance metrics for regression models are:\n",
    "\n",
    "1.Mean Squared Error (MSE)\n",
    "2.R-squared (R^2)\n",
    "3.Mean Absolute Error (MAE)\n",
    "A good Lasso regression model should have a low MSE and MAE and a high R^2 value.\n",
    "\n",
    "Step 3: Check for Overfitting\n",
    "The third step in interpreting the results of a Lasso regression model is to check for overfitting. Overfitting occurs when the model is too complex and fits the training data too closely, resulting in poor performance on the test data set.\n",
    "\n",
    "One way to check for overfitting is to compare the model’s performance on the training and test data sets. If the model performs significantly better on the training data set than the test data set, it may be overfitting.\n",
    "\n",
    "Another way to check for overfitting is to use cross-validation. Cross-validation involves splitting the data set into multiple subsets and training the model on each subset while using the remaining subsets for testing. This can help to ensure that the model is not overfitting to any particular subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3fe9a6-61f8-4337-b5aa-16a8191f1035",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a9b4a-71c4-4d4b-9c6f-4dccc2434b91",
   "metadata": {},
   "source": [
    "A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean. Shrinkage results in simple, sparse models which are easier to analyze than high-dimensional data models with large numbers of parameters.\n",
    "When λ = 0, no parameters are eliminated. The estimate is equal to the one found with linear regression.\n",
    "As λ increases, more and more coefficients are set to zero and eliminated.\n",
    "When λ = ∞, all coefficients are eliminated.\n",
    "There is a trade-off between bias and variance in resulting estimators. As λ increases, bias increases and as λ decreases, variance increases. For example, setting your tuning parameter to a low value results in a more manageable number of model parameters and lower bias, but at the expense of a much larger variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179fd8a3-4833-434f-86df-71004279fd5a",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e36721a-fd59-4a97-aae7-0957a1b0025b",
   "metadata": {},
   "source": [
    "Lasso and Ridge regression models are linear regression models with a regularization term added to the cost function. This regularization term helps to prevent overfitting by penalizing large weights.Lasso regression, or Least Absolute Shrinkage and Selection Operator, adds a term to the cost function that is the absolute value of the weights, multiplied by a constant. This causes some weights to become exactly zero, effectively eliminating them from the model and performing feature selection. This is useful when working with high-dimensional data or when you have a large number of potentially irrelevant features.Ridge regression, on the other hand, adds a term to the cost function that is the square of the weights, multiplied by a constant. This causes all weights to be small, but none of them become exactly zero. This is useful when you have a small number of features, but they are highly correlated.In general, Lasso is better at feature selection and Ridge is better at preventing overfitting. However, both methods have their own advantages and disadvantages and the choice between them depends on the specific problem you are trying to solve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa108fe3-6ac2-4d02-b319-7366e85e1845",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3c67a-a87e-4617-be8f-037f512a7ec0",
   "metadata": {},
   "source": [
    "Difference between lasso and ridge:-\n",
    "Ridge Regression:-\n",
    "1.Shrinks the coefficients toward zero\n",
    "2.Adds a penalty term proportional to the sum of squared coefficients\n",
    "3.Does not eliminate any features\n",
    "4.Suitable when all features are importantly\n",
    "5.More computationally efficient\n",
    "6.Requires setting a hyperparameter\n",
    "7.Performs better when there are many small to medium-sized coefficients\n",
    "Lasso Regression:-\n",
    "1.Encourages some coefficients to be exactly zero\n",
    "2.Adds a penalty term proportional to the sum of absolute values of coefficients\n",
    "3.Can eliminate some features\n",
    "4.Suitable when some features are irrelevant or redundant\n",
    "5.Less computationally efficient\n",
    "6.Requires setting a hyperparameter\n",
    "7.Performs better when there are a few large coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22fa8d8-0aeb-4fc8-a79f-5f86c206f2a7",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c0b44-3f04-4114-8a3c-f31acd12634f",
   "metadata": {},
   "source": [
    "Luckily, because of LASSO's built-in variable selection, it can handle some multicollinearity without sacrificing interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8188ed6-c8f7-4f28-b66a-a3d7b2a35564",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ff49f-db52-4015-ba2b-c4dd490fe1c7",
   "metadata": {},
   "source": [
    "he best cross-validation score is obtained for the 0.4 value of lambda. This is your optimal value of lambda. This is how we choose the estimated best model with optimal hyper-parameter values. Use this same process with different types of algorithms like Ridge, LASSO, Elastic-Net, Random Forests, and Boosted trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
