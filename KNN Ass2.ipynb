{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e544194-5060-4cfe-b295-15f53ad96370",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e79ee-1f2d-49b8-9373-670397fce9b2",
   "metadata": {},
   "source": [
    "The main difference between the Euclidean distance metric and the Manhattan distance metric in K-Nearest Neighbors (KNN) is how they measure the distance between data points in a feature space.\n",
    "\n",
    "Euclidean Distance:\n",
    "\n",
    "Euclidean distance, also known as L2 distance, calculates the straight-line distance between two points in a multidimensional space. It uses the Pythagorean theorem to compute the distance.\n",
    "Mathematically, the Euclidean distance between two points (x1, y1) and (x2, y2) in a 2D space is given by:\n",
    "sqrt((x1 - x2)^2 + (y1 - y2)^2)\n",
    "In a higher-dimensional space, this extends to more dimensions, and it's the square root of the sum of squared differences between corresponding coordinates.\n",
    "Manhattan Distance:\n",
    "\n",
    "Manhattan distance, also known as L1 distance or city block distance, calculates the distance between two points by summing the absolute differences of their coordinates. It's called \"Manhattan\" distance because it's akin to the distance a taxi would travel on a grid-like road system in a city, where you can only travel along the grid lines.\n",
    "Mathematically, the Manhattan distance between two points (x1, y1) and (x2, y2) in a 2D space is given by:\n",
    "|x1 - x2| + |y1 - y2|\n",
    "In higher-dimensional spaces, you sum the absolute differences for each dimension.\n",
    "How this difference affects the performance of a KNN classifier or regressor:\n",
    "\n",
    "Sensitivity to Scale:\n",
    "\n",
    "Euclidean distance considers the magnitude and direction of differences in all dimensions, which makes it sensitive to differences in scale between features. If one feature has a significantly larger range or magnitude than another, it can dominate the distance computation.\n",
    "Manhattan distance, on the other hand, only considers the absolute differences in each dimension. It is less sensitive to variations in scale, making it more suitable when features have different units or scales.\n",
    "Feature Importance:\n",
    "\n",
    "The choice between Euclidean and Manhattan distance can affect how different features contribute to the distance measurement. In Euclidean distance, features with large differences can have a larger impact, potentially overshadowing the importance of other features. In Manhattan distance, all features contribute equally to the distance calculation.\n",
    "Performance:\n",
    "\n",
    "The choice between these distance metrics should be based on the specific characteristics of your dataset and the nature of your problem. In some cases, Euclidean distance might perform better when the data naturally forms clusters in a spherical or elliptical shape, while Manhattan distance might be more suitable when the data clusters align with the coordinate axes.\n",
    "Ultimately, the choice of distance metric in KNN depends on the data distribution and the problem at hand. It's common practice to try both metrics and potentially use cross-validation to determine which one works better for a particular dataset and task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c766d76c-a332-46d4-b7e5-9e98ca14ca05",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98493e83-22ad-443d-8d2a-e01bc3c6c023",
   "metadata": {},
   "source": [
    "The value of optimum K totally depends on the dataset that you are using. The best value of K for KNN is highly data-dependent. In different scenarios, the optimum K may vary. It is more or less hit and trail method.\n",
    "\n",
    "You need to maintain a balance while choosing the value of K in KNN. K should not be too small or too large. \n",
    "\n",
    "A small value of K means that noise will have a higher influence on the result. \n",
    "\n",
    "Larger the value of K, higher is the accuracy. If K is too large, you are under-fitting your model. In this case, the error will go up again. So, at the same time you also need to prevent your model from under-fitting. Your model should retain generalization capabilities otherwise there are fair chances that your model may perform well in the training data but drastically fail in the real data. Larger K will also increase the computational expense of the algorithm.\n",
    "\n",
    "There is no one proper method of estimation of K value in KNN. No method is the rule of thumb but you should try considering following suggestions:\n",
    "\n",
    "1.Square Root Method: Take square root of the number of samples in the training dataset.\n",
    "\n",
    "2.Cross Validation Method: We should also use cross validation to find out the optimal value of K in KNN. Start with K=1, run cross validation (5 to 10 fold),  measure the accuracy and keep repeating till the results become consistent. \n",
    "\n",
    "K=1, 2, 3... As K increases, the error usually goes down, then stabilizes, and then raises again. Pick the optimum K at the beginning of the stable zone. This is also called Elbow Method.\n",
    "\n",
    "3.Domain Knowledge also plays a vital role while choosing the optimum value of K.\n",
    "\n",
    "4.K should be an odd number.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe8ab4-aaaf-4d5d-aca0-defc5f6e1824",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c72871a-3403-4650-87f1-ba963f29b779",
   "metadata": {},
   "source": [
    "The choice of distance metric in a K-Nearest Neighbors (KNN) classifier or regressor can significantly affect its performance. Different distance metrics emphasize different aspects of the data, and the selection should be made based on the characteristics of the dataset and the problem at hand. Here's how the choice of distance metric can impact KNN performance and when to use one metric over the other:\n",
    "\n",
    "Euclidean Distance:\n",
    "\n",
    "When to Use:\n",
    "Euclidean distance is often a good choice when the data naturally forms clusters in a spherical or elliptical shape.\n",
    "It works well when the feature scales are relatively consistent, and there is no significant variation in feature importance.\n",
    "Impact:\n",
    "Sensitive to scale: Euclidean distance is sensitive to differences in the magnitudes of features. Features with large scales can dominate the distance computation, potentially leading to suboptimal results if not properly scaled.\n",
    "Sensitive to outliers: Outliers with extreme values can have a substantial influence on the distance calculations, potentially leading to incorrect classifications or predictions.\n",
    "Considerations:\n",
    "Consider standardizing or normalizing features to mitigate scale sensitivity.\n",
    "Robustness to outliers can be improved by using robust distance metrics or robust KNN variants.\n",
    "Manhattan Distance (L1 Distance):\n",
    "\n",
    "When to Use:\n",
    "Manhattan distance is a good choice when the data distribution is more grid-like or when you want to downplay the influence of outliers.\n",
    "It is less sensitive to variations in feature scales, making it suitable for datasets with features of different units or scales.\n",
    "Impact:\n",
    "Less sensitive to scale: Manhattan distance considers the absolute differences in each dimension, making it less sensitive to feature scale discrepancies.\n",
    "Less affected by outliers: Outliers have a reduced impact due to the linear, non-squared nature of the metric.\n",
    "Considerations:\n",
    "Suitable for situations where all features are expected to contribute equally to the similarity calculation.\n",
    "May work better in scenarios where the data clusters align with the coordinate axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75815a27-2ad4-4c4f-a8d4-b3626e9e441c",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e052243e-6f9a-4ad9-9761-835896a4ed28",
   "metadata": {},
   "source": [
    "1.Understand the parameters: The main hyperparameter to tune in k-nearest neighbors is k, the number of neighbors to consider. Other parameters include distance metrics, weights, and algorithm types.\n",
    "2.Select a distance metric: Choose the right distance metric to measure the similarity between the data points. Common distance metrics include Euclidean, Manhattan, and cosine distance.\n",
    "3.Select an appropriate value for k: Selecting a value for k is crucial in k-nearest neighbors. A larger value of k provides a smoother decision boundary but may not be suitable for all datasets. A smaller value of k may lead to overfitting.\n",
    "4.Choose an algorithm type: k-nearest neighbors has two algorithm types: brute-force and tree-based. Brute-force algorithm computes the distances between all pairs of points in the dataset while tree-based algorithm divides the dataset into smaller parts.\n",
    "5.Cross-validation: Cross-validation is a technique used to validate the performance of the model. It involves splitting the dataset into training and testing sets and evaluating the model's performance on the testing set.\n",
    "6.Grid search: Grid search is a hyperparameter tuning technique that involves testing a range of values for each hyperparameter to find the best combination of values.\n",
    "7.Random search: Random search is another hyperparameter tuning technique that randomly selects a combination of hyperparameter values to test.\n",
    "8.Bias-variance tradeoff: k-nearest neighbors is prone to overfitting due to the high variance in the model. Regularization techniques such as L1 and L2 regularization can be used to mitigate overfitting.\n",
    "9.Data preprocessing: Data preprocessing plays a crucial role in k-nearest neighbors. Scaling the data using techniques such as normalization and standardization can improve the model's performance. Outlier removal and feature selection can also help improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7a546-0af2-441e-a213-ed442e0e2dd5",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404bab4c-0487-47fe-be2c-c27ebfae354b",
   "metadata": {},
   "source": [
    "The size of the training set can have a significant impact on the performance of a K-Nearest Neighbors (KNN) classifier or regressor. The key considerations regarding training set size are as follows:\n",
    "\n",
    "Impact of Training Set Size on KNN Performance:\n",
    "\n",
    "Overfitting vs. Underfitting:\n",
    "\n",
    "With a small training set, KNN is more likely to overfit the data. It can be overly sensitive to noise, outliers, and local variations in the training data, resulting in poor generalization to new, unseen data.\n",
    "With a large training set, KNN is less prone to overfitting. It becomes more robust to noise and is more likely to capture the underlying patterns in the data, leading to better generalization.\n",
    "Computational Complexity:\n",
    "\n",
    "As the size of the training set increases, the computational cost of KNN also increases. Each prediction or classification requires calculating distances to a larger number of training examples, which can be computationally expensive, especially for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd3228-a34d-45f6-a4f5-840395035278",
   "metadata": {},
   "source": [
    "Optimizing the Size of the Training Set:\n",
    "\n",
    "Cross-Validation:\n",
    "\n",
    "Use techniques like cross-validation to assess the impact of different training set sizes on the model's performance. Cross-validation helps you estimate how well your model is likely to perform on unseen data for various training set sizes.\n",
    "Collect More Data:\n",
    "\n",
    "In cases where the training set is small, one solution is to collect more data. A larger and more diverse dataset can help KNN perform better, especially if the small training set is not representative of the entire population.\n",
    "Data Augmentation:\n",
    "\n",
    "If collecting more data is not feasible, data augmentation techniques can be used to artificially increase the effective size of the training set. These techniques involve creating new examples by applying transformations or perturbations to existing data points, introducing variations without additional data collection.\n",
    "Feature Selection or Dimensionality Reduction:\n",
    "\n",
    "If the training set is small relative to the number of features, consider feature selection or dimensionality reduction methods to reduce the dimensionality of the data. This can help mitigate the curse of dimensionality and make KNN more effective with a small training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd599b-4284-438d-9d90-1e78a41367ec",
   "metadata": {},
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68954788-aa1c-4433-baeb-05725b5f1dda",
   "metadata": {},
   "source": [
    "Disadvantages of KNN\n",
    "\n",
    "1.Does not work well with large dataset: In large datasets, the cost of calculating the distance between the new point and each existing points is huge which degrades the performance of the algorithm.\n",
    "\n",
    "2.Does not work well with high dimensions: The KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate the distance in each dimension.\n",
    "\n",
    "3.Need feature scaling: We need to do feature scaling (standardization and normalization) before applying KNN algorithm to any dataset. If we don't do so, KNN may generate wrong predictions.\n",
    "\n",
    "4.Sensitive to noisy data, missing values and outliers: KNN is sensitive to noise in the dataset. We need to manually impute missing values and remove outliers.\n",
    "5.As the number of features increases, KNN's performance decreases due to the curse of dimensionality; this can be remedied by using feature selection, dimensionality reduction, or weighting techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
