{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2047a431-b619-4476-a785-ff369bb766eb",
   "metadata": {},
   "source": [
    "Q1. What is Elastic Net Regression and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b59319-5294-45a4-adfd-276c2b30685d",
   "metadata": {},
   "source": [
    "Elastic net linear regression uses the penalties from both the lasso and ridge techniques to regularize regression models. The technique combines both the lasso and ridge regression methods by learning from their shortcomings to improve the regularization of statistical models.\n",
    "The elastic net method improves lasso’s limitations, i.e., where lasso takes a few samples for high dimensional data. The elastic net procedure provides the inclusion of “n” number of variables until saturation. If the variables are highly correlated groups, lasso tends to choose one variable from such groups and ignore the rest entirely.This method, therefore, subjects the coefficients to two types of shrinkages. The double shrinkage from the naïve version of the elastic net causes low efficiency in predictability and high bias. To correct for such effects, the coefficients are rescaled by multiplying them by (1+λ2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca650d-d641-4ea7-b19c-0b2d84316f5d",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal values of the regularization parameters for Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39ddad6-4b4c-47d2-8393-0ac8f0ce5dc8",
   "metadata": {},
   "source": [
    "Elastic Net first emerged as a result of critique on lasso, whose variable selection can be too dependent on data and thus unstable. The solution is to combine the penalties of ridge regression and lasso to get the best of both worlds. Elastic Net aims at minimizing the following loss function:"
   ]
  },
  {
   "attachments": {
    "e37a59fd-1362-402f-82ef-c45deab844b2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "98c33a31-fb02-4c77-a0f1-fc4197879549",
   "metadata": {},
   "source": [
    "![image.png](attachment:e37a59fd-1362-402f-82ef-c45deab844b2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edd5308-9b3f-4336-ad48-17965cb5da20",
   "metadata": {},
   "source": [
    "where α is the mixing parameter between ridge (α = 0) and lasso (α = 1).\n",
    "\n",
    "Now, there are two parameters to tune: λ and α. The glmnet package allows to tune λ via cross-validation for a fixed α, but it does not support α-tuning, so we will turn to caret for this job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a77ab-c59f-4efb-9406-b46144b2e853",
   "metadata": {},
   "source": [
    "Q3. What are the advantages and disadvantages of Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5045d9-d7e4-47c0-a2f9-946bdab6245e",
   "metadata": {},
   "source": [
    "Advantages:-\n",
    "One of the benefits of elastic net is that it can handle multicollinearity, which is when some predictors are highly correlated with each other. Lasso can suffer from instability and inconsistency when there is multicollinearity, as it may arbitrarily select one predictor over another. Ridge can handle multicollinearity better, but it may keep too many predictors that are not relevant. Elastic net can overcome these problems by selecting a subset of predictors that are correlated, but not redundant.\n",
    "Another benefit of elastic net is that it can reduce overfitting, which is when the model fits the training data too well, but performs poorly on new or unseen data. Lasso and ridge can also reduce overfitting by adding regularization, but elastic net can do it more effectively by combining the benefits of both methods. Elastic net can balance the bias-variance trade-off by finding a middle ground between underfitting and overfitting.\n",
    "A third benefit of elastic net is that it can perform feature selection, which is when the model identifies the most important predictors for the outcome. Lasso can also perform feature selection by setting some coefficients to zero, but it may miss some relevant predictors if there are too many of them. Ridge cannot perform feature selection, as it keeps all the predictors, but shrinks them. Elastic net can perform feature selection by setting some coefficients to zero, while keeping others that are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d1b80-2bc4-4ed2-bd5d-4551225e9929",
   "metadata": {},
   "source": [
    "Disadvantages:-\n",
    "One of the pitfalls and challenges of elastic net is that it requires tuning two hyperparameters: alpha and lambda. Hyperparameters are parameters that are not learned by the model, but need to be specified by the user. Tuning hyperparameters means finding the optimal values that minimize the error or maximize the performance of the model. Tuning hyperparameters can be time-consuming and computationally expensive, as it requires testing different combinations of values and evaluating their results.\n",
    "Another pitfall and challenge of elastic net is that it may not work well for some types of data or problems. For example, elastic net may not be suitable for high-dimensional data, where the number of predictors is much larger than the number of observations. In this case, elastic net may not be able to select the relevant features or reduce the dimensionality effectively. Elastic net may also not be suitable for non-linear problems, where the relationship between the predictors and the outcome is not linear. In this case, elastic net may not be able to capture the complexity or the interactions of the data.\n",
    "A third pitfall and challenge of elastic net is that it may not be interpretable or explainable. Interpretability and explainability are the ability to understand how the model works and why it makes certain predictions. Lasso and ridge are relatively simple and intuitive, as they have a clear relationship between the coefficients and the predictors. Elastic net is more complex and ambiguous, as it involves a combination of two penalties and two hyperparameters. Elastic net may not provide a clear or meaningful explanation of the model or its results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce314dc-88d8-455b-a943-16a51a9f7a14",
   "metadata": {},
   "source": [
    "Q4. What are some common use cases for Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9264c70-096a-4497-887c-afa633ee61e7",
   "metadata": {},
   "source": [
    "1.Financial forecasting (like house price estimates, or stock prices)\n",
    "2.Sales and promotions forecasting.\n",
    "3.Testing automobiles.\n",
    "4.Weather analysis and prediction.\n",
    "5.Time series forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cc7e0e-6ada-4d9b-8cdc-370b362bd636",
   "metadata": {},
   "source": [
    "Q5. How do you interpret the coefficients in Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27805ed1-bda2-4350-98ab-257ffe709ffd",
   "metadata": {},
   "source": [
    "The coefficients of elastic net regression represent the linear relationship between the features and the target variable, adjusted by the regularization terms. The larger the absolute value of a coefficient, the stronger the effect of the corresponding feature on the target variable. The sign of a coefficient indicates the direction of the effect: positive for positive correlation, negative for negative correlation. The coefficients that are zero indicate that the corresponding features are not relevant for the model, and they are eliminated by the lasso penalty. Therefore, you can use the coefficients of elastic net regression to rank the features by their importance and select the ones that have non-zero coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7ee5c-04b7-4936-b10a-ffc2c3fef3f7",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values when using Elastic Net Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ae489-6448-4d17-a2a2-ba5b38f80692",
   "metadata": {},
   "source": [
    "Regression imputation is a more sophisticated approach to dealing with missing data. In this approach, we use the other variables in the dataset to predict the missing values. We first create a regression model using the variables that do not have missing values. We then use this model to predict the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612f71c-4416-4110-8019-0215c09f3745",
   "metadata": {},
   "source": [
    "Q7. How do you use Elastic Net Regression for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c20608c-a07d-438e-bc41-d6e5406745de",
   "metadata": {},
   "source": [
    "eature importance is a complex question and cannot be solved using implicit selection alone. You have a very small number of observations and a large number of features which makes the procedure a bit more difficult because the risk of overfitting is high.\n",
    "\n",
    "A simple method that will avoid (some degree) of overfitting is to use a method agnostic to your model to determine feature importance. A good example is the mean decrease in accuracy (MDA) (or increase in mean squared error). The more (less) accuracy (MSE) decrease (increase) the more important the feature. A simple way to implement this method is to randomly permute a feature (such that it should have no or little signal) and see how the model performs.\n",
    "\n",
    "Feature selection should be done on the same training data as other hyperparameter tuning (in the case of elasticnet the parameters that govern the regularization loss type and amount). This ensures you (somewhat) prevent overfitting. Ideally this allows you to eliminate some features via MDA without compromising (or with improving) your score. Additionally ElasticNet's embedded feature selection will remove even more.\n",
    "\n",
    "It may be the case, however, that your best model eliminates no features. If this is true you will have to trade score for interpretability. Note I've left out some ideas based on spectral space since I get the impression you want to know what these variables are in their originating basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5999419-e871-42d0-8d25-d74b4a0cd90e",
   "metadata": {},
   "source": [
    "Q8. How do you pickle and unpickle a trained Elastic Net Regression model in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b677beed-184c-449f-97c0-3b3bbba77c9d",
   "metadata": {},
   "source": [
    "for pickle:-\n",
    "pickle.dump(obj, file, protocol=None, *, fix_imports=True, buffer_callback=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc7394-607e-4dda-8324-db94ec616085",
   "metadata": {},
   "source": [
    "for unpickle:-\n",
    "model = pickle.load(open('model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcd73f1-965d-40fa-8982-c40e31170890",
   "metadata": {},
   "source": [
    "Q9. What is the purpose of pickling a model in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8c5a3-c683-4951-86e7-2c376582bcc3",
   "metadata": {},
   "source": [
    "n Python, the “pickle” module provides a way to serialize and deserialize Python objects, including trained machine learning models. By saving a trained model using the pickle module, you can reuse the model for making predictions on new data, without having to retrain the model from scratch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
