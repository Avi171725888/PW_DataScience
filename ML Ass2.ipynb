{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb5006bb-d878-44cc-aea4-17ce8cbf14f7",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bfe59c-6e32-4e4f-a5b1-4c99bef84cf8",
   "metadata": {},
   "source": [
    "1.Underfitting in Machine Learning:-\n",
    "A statistical model or a machine learning algorithm is said to have underfitting when a model is too simple to capture data complexities. It represents the inability of the model to learn the training data effectively result in poor performance both on the training and testing data. In simple terms, an underfit model’s are inaccurate, especially when applied to new, unseen examples. It mainly happens when we uses very simple model with overly simplified assumptions. To address underfitting problem of the model, we need to use more complex models, with enhanced feature representation, and less regularization.\n",
    "2.Overfitting in Machine Learning:-\n",
    "A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise. The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models. A solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77487b4-60c8-4ad2-9e3e-711b362ac63b",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c189728d-4698-468e-8ebd-0a4218e00377",
   "metadata": {},
   "source": [
    "To reduce overfitting we use:-\n",
    "1.Hold-out\n",
    "2.Cross-validation\n",
    "3.Data augmentation\n",
    "4.Feature selection\n",
    "5.L1 / L2 regularization\n",
    "6.Remove layers / number of units per layer\n",
    "7.Dropout\n",
    "8.Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a73bf3-22cb-49d3-9aa1-a1cadd85a3d3",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ea410-7896-4954-a58f-a98dfccc6331",
   "metadata": {},
   "source": [
    "A statistical model or a machine learning algorithm is said to have underfitting when a model is too simple to capture data complexities. It represents the inability of the model to learn the training data effectively result in poor performance both on the training and testing data.\n",
    "Reasons for Underfitting:-\n",
    "The model is too simple, So it may be not capable to represent the complexities in the data.\n",
    "The input features which is used to train the model is not the adequate representations of underlying factors influencing the target variable.\n",
    "The size of the training dataset used is not enough.\n",
    "Excessive regularization are used to prevent the overfitting, which constraint the model to capture the data well.\n",
    "Features are not scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff34e3-e705-4f70-a2bc-904f4fbaffa4",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d874f7-ab32-4eda-a3cf-b81f1267afd2",
   "metadata": {},
   "source": [
    "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as a Trade-off or Bias Variance Trade-off. This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615d7cc3-8fa8-4378-a499-086844d1d8be",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f671d-98be-4a5b-82d2-01cf6ffcd48e",
   "metadata": {},
   "source": [
    "Understanding model fit is important for understanding the root cause for poor model accuracy. This understanding will guide you to take corrective steps. We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data.\n",
    "Your model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). Your model is overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.Poor performance on the training data could be because the model is too simple (the input features are not expressive enough) to describe the target well. Performance can be improved by increasing model flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409a3f7-c38c-4a99-afbb-d74ad62639a2",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe68a7b-0683-4d6e-837a-08e363e37d97",
   "metadata": {},
   "source": [
    "Understanding bias and variance, which have roots in statistics, is essential for data scientists involved in machine learning. Bias and variance are used in supervised machine learning, in which an algorithm learns from training data or a sample data set of known quantities. The correct balance of bias and variance is vital to building machine-learning algorithms that create accurate results from their models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addfc93e-38d2-4997-b7a6-f8c284638ef7",
   "metadata": {},
   "source": [
    "Bias:-\n",
    "Bias error results from simplifying the assumptions used in a model so the target functions are easier to approximate. Bias can be introduced by model selection. Data scientists conduct resampling to repeat the model building process and derive the average of prediction values. Resampling data is the process of extracting new samples from a data set in order to get more accurate results. There are a variety of ways to resample data including:\n",
    "\n",
    "1.K fold resampling, in which a given data set is split into a K number of sections, or folds, where each fold is used as a testing set.\n",
    "2.Bootstrapping, which involves iteratively resampling a dataset with replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd2c48a-3b03-4e16-b3d3-7a03774d027e",
   "metadata": {},
   "source": [
    "Variance:-\n",
    "Variance indicates how much the estimate of the target function will alter if different training data were usedExternal link:open_in_new. In other words, variance describes how much a random variable differs from its expected value. Variance is based on a single training set. Variance measures the inconsistency of different predictions using different training sets — it’s not a measure of overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427cabe-d839-4d94-aead-1c09a81d8f2f",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd546a39-a888-403f-b7e4-3cf06111d545",
   "metadata": {},
   "source": [
    "Regularization is a technique used to reduce errors by fitting the function appropriately on the given training set and avoiding overfitting. The commonly used regularization techniques are : \n",
    "\n",
    "1.Lasso Regularization – L1 Regularization\n",
    "2.Ridge Regularization – L2 Regularization\n",
    "3.Elastic Net Regularization – L1 and L2 Regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
