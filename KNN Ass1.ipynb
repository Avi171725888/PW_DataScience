{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964ad815-09fa-463e-83be-2a369ea3cb9f",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b230f8-b0fc-4739-8e1f-d99f2759ab5d",
   "metadata": {},
   "source": [
    "The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. While it can be used for either regression or classification problems, it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd3d4fb-af75-47c8-b763-166a4aae2e78",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77b3eb-eda3-4f4b-999f-38fc7fa1811c",
   "metadata": {},
   "source": [
    "There is no one proper method of estimation of K value in KNN. No method is the rule of thumb but you should try considering following suggestions:\n",
    "\n",
    "1.Square Root Method: Take square root of the number of samples in the training dataset.\n",
    "\n",
    "2.Cross Validation Method: We should also use cross validation to find out the optimal value of K in KNN. Start with K=1, run cross validation (5 to 10 fold),  measure the accuracy and keep repeating till the results become consistent. \n",
    "\n",
    "K=1, 2, 3... As K increases, the error usually goes down, then stabilizes, and then raises again. Pick the optimum K at the beginning of the stable zone. This is also called Elbow Method.\n",
    "\n",
    "3.Domain Knowledge also plays a vital role while choosing the optimum value of K.\n",
    "\n",
    "4.K should be an odd numbe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7903a9-fedd-4767-b567-464d5f8ac591",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54397e64-9988-4303-ba34-2437abcb49e3",
   "metadata": {},
   "source": [
    "1.KNN regression tries to predict the value of the output variable by using a local average.\n",
    "2.KNN classification attempts to predict the class to which the output variable belong by computing the local probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad2c07-9f87-4426-ba2c-fcfa2b28ae3e",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7dafb-8e9f-4a55-8005-9b6e3bb40632",
   "metadata": {},
   "source": [
    "The first step is to calculate the distance between the new point and each training point. There are various methods for calculating this distance, of which the most commonly known methods are â€“ Euclidian, Manhattan (for continuous), and Hamming distance (for categorical).\n",
    "\n",
    "Euclidean Distance: Euclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (y).\n",
    "Manhattan Distance: This is the distance between real vectors using the sum of their absolute difference.\n",
    "Hamming Distance: It is used for categorical variables. If the value (x) and the value (y) are the same, the distance D will be equal to 0. Otherwise D=1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a74c4c-909c-439a-bf59-b20c557155cb",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0e1c7-3993-4d15-8013-c05368148880",
   "metadata": {},
   "source": [
    "In low-dimensions, say d=5, distance between pairs of n points uniformly sampled from a unit hypercube has a wide distribution, i.e., there are pairs of points that are close and there are pairs of points that are far. In high-dimensions, say d =100, distance between pairs of n points uniformly sampled from a unit hypercube has a narrower distribution. In fact, distribution tightens with increasing d. In other words, these n points are nearly equidistant from one another at high-dimensions. This is referred to as the Curse of Dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419cbc11-c742-47da-950b-5e0ca6a34319",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1dbe48-7e89-4f6a-bdf7-928302155c25",
   "metadata": {},
   "source": [
    "The idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space. Then we use these 'k' samples to estimate the value of the missing data points. Each sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67777e69-9d95-4043-9ad1-7e28536d7a48",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1506f78-4d3e-473f-bdb1-3aa298ca30fa",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a simple and intuitive machine learning algorithm that can be used for both classification and regression tasks. Let's compare and contrast the performance of KNN as a classifier and regressor and discuss when each is better suited for different types of problems:\n",
    "\n",
    "KNN Classifier:\n",
    "\n",
    "Purpose: KNN is primarily used for classification tasks, where the goal is to predict the class or category to which a data point belongs. For example, it can be used for tasks like spam detection, image recognition, or sentiment analysis.\n",
    "\n",
    "Output: KNN classifier assigns a class label to a data point based on the majority class among its k-nearest neighbors in the feature space.\n",
    "\n",
    "Distance Metric: In classification, you typically use distance metrics like Euclidean distance to measure the similarity between data points.\n",
    "\n",
    "Hyperparameter: The key hyperparameter in KNN classification is 'k', which represents the number of neighbors to consider. The choice of 'k' can significantly impact the classifier's performance.\n",
    "\n",
    "Decision Boundary: KNN classification creates decision boundaries that can be quite complex and non-linear, making it suitable for problems where decision boundaries are intricate.\n",
    "\n",
    "Scalability: KNN classification can be computationally expensive, especially with large datasets, as it requires calculating distances between data points for every prediction.\n",
    "\n",
    "Performance: KNN classification works well when the decision boundary is not very complex and the dataset is not too large. It might not perform well when the data is high-dimensional or imbalanced.\n",
    "\n",
    "KNN Regressor:\n",
    "\n",
    "Purpose: KNN regression, on the other hand, is used for regression tasks, where the goal is to predict a continuous numeric value. For example, it can be used for tasks like predicting house prices or stock market prices.\n",
    "\n",
    "Output: KNN regressor predicts a numeric value for a data point by averaging the values of its k-nearest neighbors.\n",
    "\n",
    "Distance Metric: Similar to classification, regression also uses distance metrics to measure similarity.\n",
    "\n",
    "Hyperparameter: The key hyperparameter in KNN regression is 'k', which represents the number of neighbors to consider. The choice of 'k' can impact the smoothness of the regression function.\n",
    "\n",
    "Output Smoothness: KNN regression can produce non-linear and piecewise smooth regression functions, which can be useful for capturing complex relationships in the data.\n",
    "\n",
    "Scalability: Like KNN classification, KNN regression can be computationally expensive, particularly with large datasets, as it involves calculating distances.\n",
    "\n",
    "Performance: KNN regression can work well when the underlying relationship between input features and the target variable is not highly non-linear and when the dataset is not too large. However, it might suffer from the curse of dimensionality in high-dimensional spaces.\n",
    "\n",
    "In summary, the choice between KNN classification and regression depends on the nature of the problem you are trying to solve. Use KNN classification when you need to categorize data into distinct classes, especially in cases where the decision boundary is not too complex. Use KNN regression when you need to predict numeric values and expect the relationship between features and the target to be somewhat smooth and continuous. However, always consider the scalability and computational requirements of KNN when applying it to large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7be670-fe13-4105-90c7-8d2c015a1120",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690da13-79f4-449f-b8a9-0ae91c86ede8",
   "metadata": {},
   "source": [
    "Advantages of KNN\n",
    "\n",
    "1.No Training Period: KNN is called Lazy Learner (Instance based learning). It does not learn anything in the training period. It does not derive any discriminative function from the training data. In other words, there is no training period for it. It stores the training dataset and learns from it only at the time of making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g. SVM, Linear Regression etc.\n",
    "\n",
    "2.Since the KNN algorithm requires no training before making predictions, new data can be added seamlessly which will not impact the accuracy of the algorithm.\n",
    "\n",
    "3.KNN is very easy to implement. There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)\n",
    "\n",
    "Disadvantages of KNN\n",
    "\n",
    "1.Does not work well with large dataset: In large datasets, the cost of calculating the distance between the new point and each existing points is huge which degrades the performance of the algorithm.\n",
    "\n",
    "2.Does not work well with high dimensions: The KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate the distance in each dimension.\n",
    "\n",
    "3.Need feature scaling: We need to do feature scaling (standardization and normalization) before applying KNN algorithm to any dataset. If we don't do so, KNN may generate wrong predictions.\n",
    "\n",
    "4.Sensitive to noisy data, missing values and outliers: KNN is sensitive to noise in the dataset. We need to manually impute missing values and remove outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ba32d-7114-49c8-9437-e4502ef808f7",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a750ce-3506-4aaa-886f-8f893b311165",
   "metadata": {},
   "source": [
    "Manhattan distance, also known as taxicab distance, is a distance measure that calculates the distance between two points by summing the absolute differences of their coordinates. Euclidean distance, on the other hand, is a distance measure that calculates the straight-line distance between two points. In other words, Manhattan distance is the distance you would travel if you were driving on a grid-like street layout, while Euclidean distance is the distance \"as the crow flies.\" Mathematically, Manhattan distance is defined as the sum of the absolute differences of the coordinates, while Euclidean distance is defined as the square root of the sum of the squared differences of the coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b9d2a4-45a2-4bba-9891-6488c8f4557f",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955c64b-ce24-4d60-a8c0-5591c2de3cd7",
   "metadata": {},
   "source": [
    "Distance-based algorithms, such as the KNN, calculate the distance between data points to determine their similarity.\n",
    "\n",
    "Features with larger magnitudes can disproportionately influence the distance calculation, leading to biased results.\n",
    "\n",
    "Feature scaling addresses this issue by transforming the features to a comparable range or scale, ensuring that each feature contributes fairly to the final result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
